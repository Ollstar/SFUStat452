---
title: "Week 6 Exercises"
author: "Brad McNeney"
date: '2017-10-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We will do model selection where 
we estimate the test error of the best model
of each size by cross-validation.
The data for this exercise is the `Credit` data.
We read in the data and perform all subsets
regression. We also use the `model.matrix()` function
to create the $X$ matrix for the full regression model.
`model.matrix()` combines all the predictors
in a formula into a matrix. For categorical predictors,
it creates the dummy variables required for the regression.

```{r}
uu <- url("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv")
Credit <- read.csv(uu,row.names=1)
head(Credit,n=3)
library(leaps) # contains regsubsets()
cfits <- regsubsets(Balance ~ ., data=Credit,nvmax=11)
cfits.sum <- summary(cfits)
Xfull <- model.matrix(Balance ~ .,data=Credit)
nrow(Xfull)
ncol(Xfull)
colnames(Xfull)
head(Xfull)
```

1. Fit all possible models that include the 
intercept plus one of the non-intercept columns in
`X` and return the RSS. Confirm the
result from `regsubsets()` that the model with
`Rating` is the best.  
Hint: If `i` was the
index in a `for()` loop from 2 to 12, 
`dat <- data.frame(Balance=Credit$Balance,X=X[,i])`
would be the data you need to fit the `i-1`st model.


2. Use 10-fold cross-validation to 
estimate the test set error of the model with 
an intercept and `Rating` as the only predictor.
The following code shows how to (i) split 
the data into folds (using an easy-to-implement
strategy from the text), (ii) train the model on 
all but the first fold and (iii) estimate the 
test set error on the first fold.
You should generalize this to code that loops
over the folds, saves the estimated test error
from each fold, and then averages the test
errors over the folds. 

We will discuss the following code in class.


```{r}
# Pull the response, Balance, out of the Credit data set
Y <- Credit$Balance
#
# Pull the intercept and Rating predictor out of Xfull.
# Use the logical vector in cfits.sum$which[1,]
X <- Xfull[,cfits.sum$which[1,]]
#
# Create folds
k <- 10
set.seed(1)
folds <- sample(1:k,size=nrow(Credit),replace=TRUE)
head(folds)
table(folds) # Not of equal size, but close enough
#
# Validation on first fold:
testY <- Y[folds==1]
trainY <- Y[folds!=1]
testX <- X[folds==1,] 
trainX <- X[folds!=1,]
#
# Use lm.fit(), the "worker" function behind lm()
# to fit the model with response trainY and 
# design matrix trainX
fit <- lm.fit(trainX,trainY) 
# There is no predict() for the output of fit so 
# we use matrix multiplication to obtain
# the fitted values on the test X's
testPreds <- testX%*%fit$coefficients
validErr <- sum((testPreds - testY)^2)
```

3. Write a function called `cv` that
does the computations in part (2). 
`cv` should take 
the following arguments:
    (a) The design matrix `X`
    (b) The response `Y`
    (c) The number of folds `k`
    (d) a random seed `seed`

and return the estimated test error by k-fold 
cross validation. Test your function to make sure
it returns the same estimate as in part (2).
Then call your function two or three more
times with different seeds
to see how the estimate changes
with different choices of folds.


4. Write code to loop over the best models of 
size 1 to 11 predictors summarized in 
`cfits.sum$which` and estimate the test error 
for each model. Report the model with the best
test error and the variables in this best model.
Hint: `which.min(vec)` returns the index of
the minimum value in `vec`.


5. What model is chosen by the $C_p$
criterion? 
Hint: The $C_p$ values are in `cfits.sum$cp`.

