---
title: 'Statistics 452: Statistical Learning and Prediction'
subtitle: 'Review Part 2: Predicting a HUI score'
author: "Brad McNeney"
date: '2018-11-26'
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```

## Data

* One of the datasets given to the Stat 652 class
includes 590 explanatory variables and a 
health utilities index score, called HUIDHSI.

\scriptsize

*This derived variable is a Health Utilities Index which provides a description of an individualâ€™s overall functional health, based
on eight attributes: vision, hearing, speech, ambulation (ability to get around), dexterity (use of hands and fingers), emotion
(feelings), cognition (memory and thinking) and pain (in HUP module). The version of the index used in CCHS is adapted from
the HUI Mark 3 (HUI3). The index is designed to produce an overall health utility score. This multi-attribute utility index
produces a score ranging from 1.000 (perfect health), through 0.000 (health status equal to death) to -0.360 (health status
worse than death).*

##

* I have unzipped the data file in my copy of 
the Project652 directory on github.

\scriptsize

```{r, cache=TRUE}
hs <- read.csv("../../Project652/HStrain.csv")
```

## 

* 591 variables, grouped into 38 categories, 
indicated by the first three letters of the 
variable names:

\scriptsize

```{r}
cn <- colnames(hs)
table(substr(cn,start=1,stop=3))
head(sort(cn),n=4) # Activities of Daily Living
```

## Survey information not useful for prediction

* The variables that start with `ADM` are 
to do with administering the survey and 
are not useful for prediction.
* For example, `ADM_RNO` is a sequential
record number, `ADM_N09` indicates whether
the interview was by phone, in-person, etc.
* I will remove these.


\scriptsize

```{r}
library(dplyr)
hs <- select(hs,-starts_with("ADM"))
```


## Summary variables

* Several categories of variable have an overall 
summary score, or classification, developed by
survey experts.
* For example, the Activities of Daily Living
(ADL) variables are summarized by `ADLDCLS` 
(page 158 of data dictionary):

\scriptsize

*ADLDCLS - Instrumental & Basic Activities of Daily Living Class. -
Based on ADLDCLST and ADLDMEA. This variable is an overall summary measure of ratings of the ADL capacity-instrumental and physical dimensions.The instrument and the derived variable classification are developed from the activities of daily living component of the OARS Multidimensional Functional Assessment Questionnaire (OMFAQ). See documentation on derived variables.*

```{r}
summary(hs$ADLDCLS)
```

## Choice of summary variable

* Some sets of variables do not have a 
single score, but may have several 
that could be useful.
* For example the caregive variables `CAG`
tell us about care responsibilities (parent,
spouse, neighbor, etc.)
    + `CAGDFAP` records the frequency of
    care (rarely, monthly, weekly, daily, etc.)
    + `CAGDIAP` records frequency and number
    of hours (daily - 1 hour, daily - 3 hours, etc.)


## Making our own score

* Some groups of variables have no 
summary score.
* Can compute a few PCs from 
a group of survey questions. 
* Example, `CIH` variables (questions
about improving health).

\scriptsize

```{r,cache=TRUE}
library(FactoMineR)
res.mca <- MCA(select(hs,starts_with("CIH")))
```

##

\scriptsize

```{r}
CIHPCs <- res.mca$ind$coord[,1:4] # first 4 explain 50%
colnames(CIHPCs) <- paste("CIH",colnames(CIHPCs))
```

    
## My choices

\scriptsize

```{r}
hsred <- select(hs,
          ADLDCLS,ALCDTTM,CAGDFAP,CCCF1,CCCDCPD,
          CR1FRHC,CR2DTHC,CR2DFAR,DPSDSF,EDUDR04,
          FALG02,GENDHDI,GENDMHI,HC2FCOP,
          HUIDHSI, # response
          HUPDPAD,HWTGBMI,IN2GHH,LONDSCR,MEDF1,
          NURDHNR,PA2DSCR,SLP_02,SLSDCLS,
          SMKDSTY,SPAFPAR,starts_with("SSAD"))
hsred <- data.frame(hsred,CIHPCs)
```

## Centre and Scale

* We will apply different statistical methods, some
of which depend on scaling of the features. 
* To make all results comparable, scale the features now.
* First have to convert factors to dummy variables
with `model.matrix()`.
    + Leave out the intercept, though.

```{r}
tem <- model.matrix(HUIDHSI ~ .,data=hsred)[,-1]
X <- as.data.frame(scale(tem))
Y <- hsred$HUIDHSI  # could also scale Y, but we won't
```

## Training and test sets

* To compare methods we'll divide our 10000 observations
into training and test sets. 
* I'll go with a 70\% training set.

\scriptsize

```{r}
set.seed(123)
n.train <- 7000
train <- sample(1:nrow(hs),replace=FALSE,size=n.train)
X.train <- X[train,]; Y.train <- Y[train]
X.test <- X[-train,]; Y.test <- Y[-train]
```

## Subset selection

\scriptsize

```{r}
library(leaps)
rr <- regsubsets(X.train,Y.train,nvmax=40,
                 method="forward")
ss <- summary(rr)
pbest <- which.min(ss$bic)
pbest
# coef(rr,id=29) 
```

* Important variables are from Activities of Daily Living, general health, satisfaction with life, smoking status, the first "improve health" PC, and some others.
* Also includes `HUPDPAD`, which should have been
exlcuded from the dataset, because it is one of the 
variables used to create the response `HUIDHSI` (oops).

## Test MSE of subset regression

\scriptsize

```{r}
cols<- ss$which[pbest,-1] # don't include intercept
Xred <- as.matrix(X.test[,cols])
pred.test <- cbind(1,Xred) %*%  coef(rr,id=29)
mean((Y.test - pred.test)^2)
```

## Lasso

\scriptsize

```{r}
library(glmnet)
lambdas <- 10^{seq(from=-3,to=5,length=100)}
cv.lafit <- cv.glmnet(as.matrix(X.train),Y.train,alpha=1,lambda=lambdas)
```

##

\scriptsize

```{r}
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
```

## Lasso coefficients

* A similar (but not identical) set of non-zero
coefficients.

\tiny

```{r}
ll <- glmnet(as.matrix(X.train),Y.train,alpha=1,lambda=la.best.lam)
coef(ll)
```

##

\scriptsize

```{r}
pred.test <- predict(ll,as.matrix(X.test))
mean((Y.test-pred.test)^2)
```

## Random forests

* Computationally intensive, and computation 
grows with the number of features and observations.
* With many features and a large sample size, may need
to filter some out, or need to reduce the 
sample size while we explore which features
are important.

## Use features found by lasso or subset selection

* Could do something like the following to 
use a subset of the features.
    + Here I pick out the features chosen by lasso
    
\scriptsize

```{r}
nonz <- (as.numeric(coef(ll))!=0)[-1] # rm intercept
hsred2.train <- data.frame(HUIDHSI=Y.train,X.train[,nonz])
```


## Random forest with all features

* My laptop can handle all features, but 
I've chosen to build only 200 trees.

\scriptsize

```{r,cache=TRUE}
library(randomForest)
set.seed(1)
bb <- randomForest(X.train,y=Y.train,xtest=X.test,
          ytest=Y.test,ntree=200,
          mtry=sqrt(ncol(X.train)),importance=TRUE)
varImpPlot(bb,type=1) # HUPDPAD levels 4,5,3 important
```

##

\scriptsize

```{r}
pred.test <- bb$test$predicted
mean((Y.test - pred.test)^2)
```

## Boosting

* Remember that the number of trees and the shrinkage
factor are tuning parameters. 
* To limit computation I'm using 200 trees.
* I've left the shrinkage at its default value.

\tiny

```{r,cache=TRUE}
library(gbm)
hs.train <- data.frame(HUIDHSI=Y.train,X.train)
hboost <- gbm(HUIDHSI ~ ., data=hs.train,
              n.trees=200,distribution="gaussian") 
summary(hboost)
```

##

\scriptsize

```{r}
library(gbm)
hs.test <- data.frame(HUIDHSI=Y.test,X.test)
pred.test <- predict(hboost,newdata=hs.test, n.trees=200,type="response")
mean((Y.test-pred.test)^2)
```


## The winner

* Surprisingly, the lasso gave the lowest MSE.
    + But I didn't use many trees for random forest, and 
    made no attempt to tune boosting.
* Fit the winner to all data -- this is my $\hat{f}$ 
to predict the hold-out test data on Monday.

\tiny

```{r}
lambdas <- 10^{seq(from=-3,to=5,length=100)}
cv.lafit <- cv.glmnet(as.matrix(X),Y,alpha=1,lambda=lambdas)
la.best.lam <- cv.lafit$lambda.1se
winner <- glmnet(as.matrix(X),Y,alpha=1,lambda=la.best.lam)
coef(winner)
```